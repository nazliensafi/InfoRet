{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef59e61-ba00-450f-b23b-fa7bb2feae42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9b35879-9e9b-436c-b5d6-4520048b8eab",
   "metadata": {},
   "source": [
    "#### Goal: text preprocessing with NLTK, proofreading results\n",
    "#### Data: Reuter's Corpus Reuters-21578\n",
    "#### http://www.daviddlewis.com/resources/testcollections/reuters21578/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60481d8-e01e-4363-8a21-691443f6272a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a2c77e-d8ea-4bab-9258-ff5a2a4508a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kaveh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/kaveh/nazli/inforet/InfoRet/P1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize NLTK\n",
    "nltk.download('punkt')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255610a8-4d10-4a5c-b9da-e5a1ecd68bf6",
   "metadata": {},
   "source": [
    "#### 1. read the Reuter's collection and extract the raw text of each Reuter's news item (these are your documents) from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1820ef3c-806b-46dc-9b0a-71ec3098c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read Reuters collection and extract raw text\n",
    "def extract_raw_text_from_reuters(folder_path, num_documents):\n",
    "    raw_text_collection = []\n",
    "    document_count = 0  # Counter to keep track of the number of documents processed\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.sgm') :\n",
    "            sgml_file = os.path.join(folder_path, filename)\n",
    "\n",
    "            with open(sgml_file, 'r', encoding='latin-1') as file:\n",
    "                content = file.read()\n",
    "\n",
    "            fileContent = BeautifulSoup(content, 'html.parser')\n",
    "            docs = fileContent.find('reuters')\n",
    "            \n",
    "            while( document_count < num_documents ):\n",
    "                docId = docs['newid']\n",
    "                body_elements = docs.find_all('body')\n",
    "    \n",
    "                # Initialize a list for the bodies of the current document\n",
    "                document_bodies = []\n",
    "\n",
    "                with open( f\"raw_text_{docId}.txt\", 'w' ) as output:\n",
    "                \n",
    "                    for body_element in body_elements:\n",
    "                        raw_text = body_element.get_text()\n",
    "                        #raw_text_collection.append(raw_text)\n",
    "                        output.write(raw_text)\n",
    "                        output.write(\"\\n\")\n",
    "                \n",
    "                # Increment the document count\n",
    "                document_count += 1\n",
    "                docs = docs.find_next(\"reuters\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b8cd37-8b2f-4d91-95c5-ae7ba3197ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the reuters21578/ folder in the current repository\n",
    "folder_path = 'reuters21578/'\n",
    "\n",
    "# Number of documents to process\n",
    "num_documents = 5\n",
    "\n",
    "extract_raw_text_from_reuters(folder_path, num_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb266794-7e9c-4234-8f1b-c3a3eaa7404f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cdb56c-979f-4096-8cbe-a89b5bc30198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the text\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78606583-fdb0-461d-952e-8f84c526423b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75b72327-d488-4d8d-bd7d-2f1eb4b2dcfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer\n",
    "\n",
    "all_files = os.listdir(os.getcwd()) #list of files in the current directory\n",
    "for each_file in all_files:\n",
    "    if each_file.startswith('raw_text_'):\n",
    "        end = each_file.find('.')\n",
    "        idStr = each_file[ len( 'raw_text_' ):end]\n",
    "        id = int( idStr )\n",
    "\n",
    "        with open( each_file, 'r' ) as input:\n",
    "            output = open( f\"Tokenizer-output_{id}.txt\", 'w')\n",
    "            for line in input.readlines():\n",
    "                #print( \"line:\", line )\n",
    "                for token in tokenize_text( line ):\n",
    "                    #print(\"token:\", token)\n",
    "                    output.write( token )\n",
    "                    output.write( \"\\n\" )\n",
    "\n",
    "            output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dab948-d755-4a93-982e-2bad5764be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make text lowercase\n",
    "def make_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b83ee0f-7c9d-4908-a7d5-523f3d45a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lowercase\n",
    "\n",
    "all_files = os.listdir(os.getcwd()) #list of files in the current directory\n",
    "for each_file in all_files:\n",
    "    if each_file.startswith('Tokenizer-output_'):\n",
    "        end = each_file.find('.')\n",
    "        idStr = each_file[ len( 'Tokenizer-output_' ):end]\n",
    "        id = int( idStr )\n",
    "\n",
    "        with open( each_file, 'r' ) as input:\n",
    "            output = open( f\"Lowercased-output_{id}.txt\", 'w')\n",
    "            for line in input.readlines():\n",
    "                #print( \"line:\", line )\n",
    "                low = make_lowercase( line )\n",
    "                output.write( low )\n",
    "                #output.write( \"\\n\" )\n",
    "\n",
    "            output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeeeeb8-1390-43d4-897f-28143c6aa2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9719306-97c0-461c-80d0-086e357552b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply Porter stemmer\n",
    "def apply_porter_stemmer(token):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(token.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76d9945c-ad8d-49e0-b443-6a445d0cf8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer\n",
    "\n",
    "all_files = os.listdir(os.getcwd()) #list of files in the current directory\n",
    "for each_file in all_files:\n",
    "    if each_file.startswith('Lowercased-output_'):\n",
    "        end = each_file.find('.')\n",
    "        idStr = each_file[ len( 'Lowercased-output_' ):end]\n",
    "        id = int( idStr )\n",
    "\n",
    "        with open( each_file, 'r' ) as input:\n",
    "            output = open( f\"Stemmed-output_{id}.txt\", 'w')\n",
    "            for line in input.readlines():\n",
    "                #print( \"line:\", line )\n",
    "                stemmed = apply_porter_stemmer( line )\n",
    "                output.write( stemmed )\n",
    "                output.write( \"\\n\" )\n",
    "\n",
    "            output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6914321-e229-434d-8f6b-24fe6b942c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "849829ae-4736-41cc-a843-156fd2e6e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kaveh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#make a stopwords file\n",
    "\n",
    "# Get a list of NLTK English stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Savle stopwords used in a file\n",
    "stopwords_used_output = open('Stopwords-used-for-output.txt', 'w')\n",
    "stopwords_used_output.write('\\n'.join(stop_words))\n",
    "stopwords_used_output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6234284-41d8-4923-9295-b423e5fb5aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed did\n",
      "\n",
      "removed hasn't\n",
      "\n",
      "removed was\n",
      "\n",
      "removed hadn\n",
      "\n",
      "removed it\n",
      "\n",
      "removed any\n",
      "\n",
      "removed o\n",
      "\n",
      "removed after\n",
      "\n",
      "removed of\n",
      "\n",
      "removed if\n",
      "\n",
      "removed as\n",
      "\n",
      "removed yourselves\n",
      "\n",
      "removed can\n",
      "\n",
      "removed ours\n",
      "\n",
      "removed further\n",
      "\n",
      "removed few\n",
      "\n",
      "removed before\n",
      "removed and\n",
      "\n",
      "removed they\n",
      "\n",
      "removed to\n",
      "\n",
      "removed a\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed and\n",
      "\n",
      "removed of\n",
      "\n",
      "removed both\n",
      "\n",
      "removed is\n",
      "\n",
      "removed a\n",
      "\n",
      "removed of\n",
      "\n",
      "removed which\n",
      "\n",
      "removed a\n",
      "\n",
      "removed in\n",
      "\n",
      "removed the\n",
      "\n",
      "removed will\n",
      "\n",
      "removed be\n",
      "\n",
      "removed and\n",
      "\n",
      "removed will\n",
      "\n",
      "removed be\n",
      "\n",
      "removed by\n",
      "\n",
      "removed under\n",
      "\n",
      "removed the\n",
      "\n",
      "removed of\n",
      "\n",
      "removed a\n",
      "\n",
      "removed it\n",
      "\n",
      "removed an\n",
      "\n",
      "removed with\n",
      "\n",
      "removed the\n",
      "\n",
      "removed of\n",
      "\n",
      "removed the\n",
      "\n",
      "removed in\n",
      "\n",
      "removed an\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed in\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed having\n",
      "\n",
      "removed in\n",
      "\n",
      "removed and\n",
      "\n",
      "removed in\n",
      "\n",
      "removed is\n",
      "\n",
      "removed not\n",
      "\n",
      "removed under\n",
      "\n",
      "removed to\n",
      "\n",
      "removed on\n",
      "\n",
      "removed its\n",
      "\n",
      "removed and\n",
      "\n",
      "removed do\n",
      "\n",
      "removed to\n",
      "\n",
      "removed it\n",
      "\n",
      "removed because\n",
      "\n",
      "removed of\n",
      "\n",
      "removed the\n",
      "\n",
      "removed some\n",
      "\n",
      "removed they\n",
      "\n",
      "removed have\n",
      "\n",
      "removed its\n",
      "\n",
      "removed up\n",
      "\n",
      "removed to\n",
      "\n",
      "removed which\n",
      "\n",
      "removed has\n",
      "\n",
      "removed to\n",
      "\n",
      "removed be\n",
      "\n",
      "removed by\n",
      "\n",
      "removed the\n",
      "\n",
      "removed and\n",
      "\n",
      "removed this\n",
      "\n",
      "removed with\n",
      "\n",
      "removed other\n",
      "\n",
      "removed on\n",
      "\n",
      "removed the\n",
      "\n",
      "removed that\n",
      "\n",
      "removed has\n",
      "\n",
      "removed on\n",
      "\n",
      "removed a\n",
      "\n",
      "removed of\n",
      "\n",
      "removed its\n",
      "\n",
      "removed the\n",
      "\n",
      "removed down\n",
      "\n",
      "removed this\n",
      "\n",
      "removed after\n",
      "\n",
      "removed to\n",
      "\n",
      "removed this\n",
      "\n",
      "removed on\n",
      "\n",
      "removed the\n",
      "\n",
      "removed that\n",
      "\n",
      "removed with\n",
      "\n",
      "removed the\n",
      "\n",
      "removed of\n",
      "\n",
      "removed the\n",
      "\n",
      "removed i\n",
      "\n",
      "removed is\n",
      "\n",
      "removed under\n",
      "\n",
      "removed no\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed into\n",
      "\n",
      "removed a\n",
      "\n",
      "removed that\n",
      "\n",
      "removed will\n",
      "\n",
      "removed be\n",
      "\n",
      "removed on\n",
      "\n",
      "removed in\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed on\n",
      "\n",
      "removed it\n",
      "\n",
      "removed was\n",
      "\n",
      "removed as\n",
      "\n",
      "removed of\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed its\n",
      "\n",
      "removed on\n",
      "\n",
      "removed a\n",
      "\n",
      "removed is\n",
      "\n",
      "removed than\n",
      "\n",
      "removed and\n",
      "\n",
      "removed now\n",
      "\n",
      "removed be\n",
      "\n",
      "removed the\n",
      "\n",
      "removed at\n",
      "\n",
      "removed the\n",
      "\n",
      "removed will\n",
      "\n",
      "removed what\n",
      "\n",
      "removed we\n",
      "\n",
      "removed do\n",
      "\n",
      "removed for\n",
      "\n",
      "removed when\n",
      "\n",
      "removed if\n",
      "\n",
      "removed with\n",
      "\n",
      "removed the\n",
      "\n",
      "removed after\n",
      "\n",
      "removed it\n",
      "\n",
      "removed i\n",
      "\n",
      "removed it\n",
      "\n",
      "removed off\n",
      "\n",
      "removed as\n",
      "\n",
      "removed as\n",
      "\n",
      "removed they\n",
      "\n",
      "removed with\n",
      "\n",
      "removed and\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed they\n",
      "\n",
      "removed have\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed an\n",
      "\n",
      "removed has\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed of\n",
      "\n",
      "removed it\n",
      "\n",
      "removed most\n",
      "\n",
      "removed a\n",
      "\n",
      "removed at\n",
      "\n",
      "removed of\n",
      "\n",
      "removed it\n",
      "\n",
      "removed such\n",
      "\n",
      "removed an\n",
      "\n",
      "removed at\n",
      "\n",
      "removed a\n",
      "\n",
      "removed a\n",
      "\n",
      "removed and\n",
      "\n",
      "removed more\n",
      "\n",
      "removed to\n",
      "\n",
      "removed with\n",
      "\n",
      "removed that\n",
      "\n",
      "removed while\n",
      "\n",
      "removed they\n",
      "\n",
      "removed the\n",
      "\n",
      "removed will\n",
      "\n",
      "removed to\n",
      "\n",
      "removed over\n",
      "\n",
      "removed the\n",
      "\n",
      "removed through\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed is\n",
      "\n",
      "removed to\n",
      "\n",
      "removed over\n",
      "\n",
      "removed the\n",
      "\n",
      "removed which\n",
      "\n",
      "removed about\n",
      "\n",
      "removed in\n",
      "\n",
      "removed to\n",
      "\n",
      "removed if\n",
      "\n",
      "removed the\n",
      "\n",
      "removed is\n",
      "\n",
      "removed on\n",
      "\n",
      "removed the\n",
      "\n",
      "removed and\n",
      "\n",
      "removed as\n",
      "\n",
      "removed as\n",
      "\n",
      "removed if\n",
      "\n",
      "removed no\n",
      "\n",
      "removed for\n",
      "\n",
      "removed a\n",
      "\n",
      "removed with\n",
      "\n",
      "removed and\n",
      "\n",
      "removed he\n",
      "\n",
      "removed that\n",
      "\n",
      "removed any\n",
      "\n",
      "removed not\n",
      "\n",
      "removed up\n",
      "\n",
      "removed in\n",
      "\n",
      "removed the\n",
      "\n",
      "removed with\n",
      "\n",
      "removed other\n",
      "\n",
      "removed to\n",
      "\n",
      "removed more\n",
      "\n",
      "removed than\n",
      "\n",
      "removed if\n",
      "\n",
      "removed to\n",
      "\n",
      "removed its\n",
      "\n",
      "removed the\n",
      "\n",
      "removed they\n",
      "\n",
      "removed the\n",
      "\n",
      "removed will\n",
      "\n",
      "removed be\n",
      "\n",
      "removed to\n",
      "\n",
      "removed was\n",
      "\n",
      "removed to\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed the\n",
      "\n",
      "removed through\n",
      "\n",
      "removed as\n",
      "\n",
      "removed i\n",
      "\n",
      "removed and\n",
      "\n",
      "removed have\n",
      "\n",
      "removed after\n",
      "\n",
      "removed for\n",
      "\n",
      "removed and\n",
      "\n",
      "removed after\n",
      "\n",
      "removed for\n",
      "\n",
      "removed after\n",
      "\n",
      "removed after\n",
      "\n",
      "removed after\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stop words\n",
    "\n",
    "all_files = os.listdir(os.getcwd()) #list of files in the current directory\n",
    "for each_file in all_files:\n",
    "    if each_file.startswith('Stemmed-output_'):\n",
    "        end = each_file.find('.')\n",
    "        idStr = each_file[ len( 'Stemmed-output_' ):end]\n",
    "        id = int( idStr )\n",
    "\n",
    "        with open( each_file, 'r' ) as input:\n",
    "            output = open( f\"No-stopword-output_{id}.txt\", 'w')\n",
    "            for line in input.readlines():\n",
    "                if line.strip() not in stop_words:\n",
    "                    output.write( line )\n",
    "                    #output.write( \"\\n\" )\n",
    "                else:\n",
    "                    print( f\"removed {line}\" )\n",
    "\n",
    "            output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1e257-9837-433f-b751-ba8f56b55f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
